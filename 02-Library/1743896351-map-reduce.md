---
aliases:
  - map-reduce
tags:
  - #resource
created_date: "2025-04-05"
link: "[[1743811421-procesamiento-de-datos-masivos|Procesamiento De Datos Masivos]]"
---

# map-reduce

## Summary


## Key Ideas
### What is the MapReduce programming model?

- Is a programming model designed for efficiently processing large datasets
- It has three steps:
  - Mapping:
    - Used to **transform** input data into key-value pairs.
    - [[Spark]] uses [[1743896431-resilient-distributed-dataset|RDDs]] or DataFrames for processing.
    - `rdd.map(x => (x, 1))`
  - Shuffling:

    - Redistributes and groups data based on keys.
    - [[Spark]] handles Shuffling automatically.
    - `rdd.reduceByKey((a, b) => a + b)`

  - Reducing:
	  - **Aggregates** values associated with the same key.
	  - `rdd.reduceByKey((a, b) => a + b)`

## What is [[RDD]]?

- Resilient Distributed Dataset.
- It's and immutable and unstructured distributed collection of data that can be processed in parallel across nodes.
- It's fault tolerant and can be cached in memory for faster access.

## Related Links
- [[ ]]

## Source
[text](url) 

